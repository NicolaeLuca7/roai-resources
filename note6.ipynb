{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "from PIL import Image\n",
    "import torch\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import torch.utils.data as tdata\n",
    "from tqdm import tqdm\n",
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "from transformers.models.detr.feature_extraction_detr import rgb_to_id\n",
    "from transformers import AutoImageProcessor, AutoModelForImageClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_value = 7\n",
    "torch.manual_seed(seed_value)\n",
    "if torch.cuda.is_available():\n",
    "  torch.cuda.manual_seed(seed_value)\n",
    "  torch.cuda.manual_seed_all(seed_value)\n",
    "np.random.seed(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device= torch.device(\"cuda:0\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install --upgrade pyarrow\n",
    "# %pip install -qq -U diffusers datasets transformers accelerate ftfy pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"keremberke/excavator-detector\", \"full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(model.roi_heads.box_predictor.cls_score.in_features, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(tdata.Dataset):\n",
    "    def __init__(self,dataset):\n",
    "\n",
    "        self.image=[]\n",
    "        self.type=[]\n",
    "        self.bbox=[]\n",
    "        for id in range(len(dataset['image'])):\n",
    "            self.image.append(dataset['image'][id])\n",
    "            self.type.append(dataset['objects'][id]['category'][0]-1)\n",
    "            box=np.array(dataset['objects'][id]['bbox'][0])\n",
    "            self.bbox.append(box)\n",
    "                \n",
    "        self.len=len(self.image)\n",
    "    \n",
    "    def __getitem__(self, id):\n",
    "        return  torch.from_numpy(np.array(self.image[id])).type(dtype=torch.float),torch.from_numpy(self.bbox[id]).type(dtype=torch.float)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set=Dataset(dataset['train'][0:104])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, boxes = torch.rand(1, 3, 600, 1200), torch.tensor([0,0,1,1])\n",
    "v=[{'labels':torch.tensor([[1]],dtype=torch.int64).to(device),'boxes':torch.tensor([[0,0,1,1]],dtype=torch.float32)}]\n",
    "\n",
    "#different outputs\n",
    "model.train()\n",
    "out=model(images,[{'labels':torch.tensor([1],dtype=torch.int64),'boxes':torch.tensor([[0,0,1,1]],dtype=torch.float32)}])\n",
    "i=0\n",
    "\n",
    "model.eval()\n",
    "out=model(images,[{'labels':torch.tensor([1],dtype=torch.int64),'boxes':torch.tensor([[0,0,1,1]],dtype=torch.float32)}])\n",
    "i=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 640, 640, 3])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "torch.flip(train_set[0][0].unsqueeze(0), dims=[0,1]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set[0][1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
